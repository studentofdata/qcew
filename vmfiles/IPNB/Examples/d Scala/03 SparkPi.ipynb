{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkPi\n",
    "\n",
    "We now turn to doing some lightweight Spark stuff in Scala. \n",
    "\n",
    "This is the same  [SparkPi program]([https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala) available in the Scala section of Spark examples.\n",
    "\n",
    "It computes $\\pi$ up to some digits by computing the area of the unit circle (which is $A = \\pi  r^2 = \\pi$). To do so, the unit square is repeteadly sampled (by randomly taking points from it) and the points falling within the unit circle are counted; the result is an approximation of the desired area. See the [wikipedia page](https://en.wikipedia.org/wiki/Approximations_of_%CF%80#Summing_a_circle.27s_area) for further explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As a complete program\n",
    "We could execute the complete SparkPI Scala snippet. Let's take a look at the original source:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```Scala\n",
    "import scala.math.random\n",
    "import org.apache.spark._\n",
    "\n",
    "/** Computes an approximation to pi */\n",
    "object SparkPi {\n",
    "  def main(args: Array[String]) {\n",
    "    val conf = new SparkConf().setAppName(\"Spark Pi\")\n",
    "    val spark = new SparkContext(conf)\n",
    "    val slices = if (args.length > 0) args(0).toInt else 2\n",
    "    val n = 100000 * slices\n",
    "    val count = sc.parallelize(1 to n, slices).map { i =>\n",
    "      val x = random * 2 - 1\n",
    "      val y = random * 2 - 1\n",
    "      if (x*x + y*y < 1) 1 else 0\n",
    "    }.reduce(_ + _)\n",
    "    println(\"Pi is roughly \" + 4.0 * count / n)\n",
    "    spark.stop()\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be run with a small modification: the Scala kernel this ntebook is connected to already provides a `SparkContext` ready for us to use, in the `sc` variable. \n",
    "So we do not need to create a new context by executing\n",
    "```Scala\n",
    "    val spark = new SparkContext(conf)\n",
    "```\n",
    " ... and indeed we should **not** do so, because the operation tries to create *another* context, and unless the kernel is running with the configuration property *spark.driver.allowMultipleContexts* set to `true`, it will fail. \n",
    " \n",
    " So let's use that already-available context, and redefine the program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scala.math.random\n",
    "import org.apache.spark._\n",
    "\n",
    "/** Computes an approximation to pi */\n",
    "object SparkPi {\n",
    "  def main(args: Array[String]) {\n",
    "    val conf = sc.getConf /* <--- we use the context the kernel has already created */\n",
    "    conf.setAppName( \"Spark Pi\" )\n",
    "    val slices = if (args.length > 0) args(0).toInt else 2\n",
    "    val n = 100000 * slices\n",
    "    val count = sc.parallelize(1 to n, slices).map { i =>\n",
    "      val x = random * 2 - 1\n",
    "      val y = random * 2 - 1\n",
    "      if (x*x + y*y < 1) 1 else 0\n",
    "    }.reduce(_ + _)\n",
    "    println(\"Pi is roughly \" + 4.0 * count / n)\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we execute the defined program by calling it with the required arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r",
      "Pi is roughly 3.14318\n"
     ]
    }
   ],
   "source": [
    "SparkPi.main( Array(\"2\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As a notebook\n",
    "\n",
    "Now we do the same, but in a more notebook-friendly shape by splitting the program into cells to be computed sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Imports we need\n",
    "import scala.math.random\n",
    "import org.apache.spark._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "org.apache.spark.SparkConf@19ebfef4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Set out application name\n",
    "val conf = sc.getConf\n",
    "conf.setAppName( \"Spark Pi\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val slices = 2\n",
    "val n = 100000 * slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val count = sc.parallelize(1 to n, slices).map { i =>\n",
    "      val x = random * 2 - 1\n",
    "      val y = random * 2 - 1\n",
    "      if (x*x + y*y < 1) 1 else 0\n",
    "    }.reduce(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.13922\n"
     ]
    }
   ],
   "source": [
    "println(\"Pi is roughly \" + 4.0 * count / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "",
   "name": "spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
